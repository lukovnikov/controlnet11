{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ccae926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6303df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c5150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample([1,2,3], 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2f483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import COCODataset, COCODataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9057bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/lukovdg1/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/lukovdg1/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 1000/1000 [10.7s elapsed, 0s remaining, 106.2 samples/s]      \n",
      "Dataset 'coco-2017-validation-1000' created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:09<00:00, 110.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 83, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. check dataloader\n",
    "ds = COCODataset(split=\"valid\", max_samples=1000)\n",
    "dl = COCODataLoader(ds, batch_size={384: 5, 448:4, 512: 4}, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db23bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([5, 3, 384, 384])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([4, 3, 448, 448])\n",
      "torch.Size([1, 3, 448, 448])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([4, 3, 512, 512])\n",
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in iter(dl):\n",
    "    print(batch[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a381ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCLIPTextEmbedder(FrozenCLIPEmbedder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n",
    "\n",
    "    def forward(self, fullprompt):\n",
    "        fullprompt, batsize = fullprompt if isinstance(fullprompt, tuple) else (fullprompt, 1)\n",
    "        \n",
    "        layer_idses = []\n",
    "        text_embeddingses = []\n",
    "        token_idses = []\n",
    "        \n",
    "        layertexts = fullprompt.split(\"\\n|\")\n",
    "        \n",
    "        # encode layers separately\n",
    "        for i, pos_prompt in enumerate(layertexts):\n",
    "            if i == 0:\n",
    "                token_ids, layer_ids = _tokenize_annotated_prompt(pos_prompt.strip(), self.tokenizer)\n",
    "                global_len = token_ids.shape[0]\n",
    "                global_bos_pos = 0\n",
    "                global_eos_pos = torch.nonzero(token_ids == self.tokenizer.eos_token_id)[0][0]\n",
    "            else:\n",
    "                pos_prompt = pos_prompt.strip()\n",
    "                token_ids = self.tokenizer([pos_prompt], return_tensors=\"pt\",\n",
    "                                        max_length=self.max_length, return_overflowing_tokens=False,\n",
    "                                        truncation=True)[\"input_ids\"][0]\n",
    "                layer_ids = torch.tensor([i] * token_ids.shape[0])\n",
    "            outputs = self.transformer(input_ids=token_ids[None].to(self.device), output_hidden_states=self.layer==\"hidden\")\n",
    "            if self.layer == \"last\":\n",
    "                z = outputs.last_hidden_state\n",
    "            elif self.layer == \"pooled\":\n",
    "                z = outputs.pooler_output[:, None, :]\n",
    "            else:\n",
    "                z = outputs.hidden_states[self.layer_idx]\n",
    "                \n",
    "            layer_idses.append(layer_ids)\n",
    "            token_idses.append(token_ids)\n",
    "            text_embeddingses.append(z)\n",
    "            \n",
    "        layer_ids = torch.cat(layer_idses, 0)[None].repeat(batsize, 1)\n",
    "        token_ids = torch.cat(token_idses, 0)[None].repeat(batsize, 1)\n",
    "        text_embeddings = torch.cat(text_embeddingses, 1).repeat(batsize, 1, 1)\n",
    "        global_prompt_mask = torch.zeros_like(token_ids)\n",
    "        global_bos_eos_mask = torch.zeros_like(global_prompt_mask)\n",
    "        global_prompt_mask[:, :global_len] = 1\n",
    "        global_bos_eos_mask[:, global_bos_pos] = 1\n",
    "        global_bos_eos_mask[:, global_eos_pos] = 1\n",
    "        \n",
    "        ret = CustomTextConditioning(text_embeddings, layer_ids.to(self.device), token_ids.to(self.device),\n",
    "                                     global_prompt_mask.to(self.device), global_bos_eos_mask.to(self.device))\n",
    "        return ret\n",
    "        # return text_embeddings, layer_ids.to(self.device), token_ids.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc49031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create model\n",
    "model_name = 'control_v11p_sd15_seg'\n",
    "model = create_model(f'./models/{model_name}.yaml').cpu()\n",
    "# load main weights\n",
    "model.load_state_dict(load_state_dict('./models/v1-5-pruned.ckpt', location='cuda'), strict=False)\n",
    "# load controlnet weights\n",
    "model.load_state_dict(load_state_dict(f'./models/{model_name}.pth', location='cuda'), strict=False)\n",
    "model = model.cuda()\n",
    "\n",
    "# cast text encoder to our own\n",
    "model.cond_stage_model.__class__ = MyCLIPTextEmbedder\n",
    "\n",
    "for _module in model.model.diffusion_model.modules():\n",
    "    if _module.__class__.__name__ == \"CrossAttention\":\n",
    "        _module.__class__.forward = custom_forward\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ControlNetV11",
   "language": "python",
   "name": "control-v11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
